test.iloc[:,8:9]

print(result)
              precision    recall  f1-score   support

         bad     0.8010    1.0000    0.8895    980884
        good     0.8000    0.0000    0.0000    243724

    accuracy                         0.8010   1224608
   macro avg     0.8005    0.5000    0.4448   1224608
weighted avg     0.8008    0.8010    0.7125   1224608



test.iloc[:,9:10]
print(result)
              precision    recall  f1-score   support

         bad     0.8025    0.9999    0.8904    981772
        good     0.9518    0.0051    0.0102    242836

    accuracy                         0.8027   1224608
   macro avg     0.8772    0.5025    0.4503   1224608
weighted avg     0.8321    0.8027    0.7159   1224608

test.iloc[:, 22:23]
 print(result)
              precision    recall  f1-score   support     

         bad     0.9965    0.9874    0.9919    980492
        good     0.9511    0.9862    0.9684    244116

    accuracy                         0.9872   1224608
   macro avg     0.9738    0.9868    0.9802   1224608
weighted avg     0.9875    0.9872    0.9872   1224608
When using count as the predictor variable, i was able to improve the model across the board 

dt = tree.DecisionTreeClassifier(max_features=("auto"),splitter="random")
 print(result)
              precision    recall  f1-score   support

         bad     0.9968    0.9870    0.9919    981439
        good     0.9497    0.9871    0.9680    243169

    accuracy                         0.9871   1224608
   macro avg     0.9732    0.9871    0.9800   1224608
weighted avg     0.9874    0.9871    0.9871   1224608

DecisionTreeClassifier(random_state=(10),criterion=("entropy"),splitter="random")
print(result)
              precision    recall  f1-score   support

         bad     0.9965    0.9872    0.9919    980886
        good     0.9505    0.9862    0.9680    243722

    accuracy                         0.9870   1224608
   macro avg     0.9735    0.9867    0.9799   1224608
weighted avg     0.9874    0.9870    0.9871   1224608

